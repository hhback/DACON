{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Multi-Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel, TFAutoModelForSequenceClassification\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"DACON_235978\", name=\"tutorial\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"tutorial\")\n",
    "parser.add_argument('--image_pretrained_model', default=\"InceptionV3\", type=str)\n",
    "parser.add_argument('--image_size', default=299, type=int)\n",
    "parser.add_argument('--text_pretrained_model', default=\"roberta\", type=str)\n",
    "parser.add_argument('--text_len', default=300, type=int)\n",
    "parser.add_argument('--optimizer', default=\"sgd\", type=str)\n",
    "parser.add_argument('--learning_rate', default=0.002, type=float)\n",
    "parser.add_argument('--loss', default='cc', type=str)\n",
    "parser.add_argument('--label_smoothing', default=0.1, type=float)\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--validation_split', default=0.2, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "wandb.config.update(args)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "image_pretrained_model = args.image_pretrained_model\n",
    "image_size = args.image_size\n",
    "text_pretrained_model = args.text_pretrained_model\n",
    "text_len = args.text_len\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = args.epochs\n",
    "VALIDATION_SPLIT = args.validation_split\n",
    "SEED = args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_pretrained_model == \"InceptionV3\":\n",
    "    image_pretrained_model = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(image_size, image_size, 3),\n",
    "        pooling=\"avg\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.text_pretrained_model == \"roberta\":\n",
    "    text_pretrained_model = \"klue/roberta-large\"\n",
    "if args.text_pretrained_model == \"kosroberta\":\n",
    "    text_pretrained_model = \"jhgan/ko-sroberta-multitask\"\n",
    "if args.text_pretrained_model == \"funnel\":\n",
    "    text_pretrained_model = \"kykim/funnel-kor-base\"\n",
    "if args.text_pretrained_model == \"electra\":\n",
    "    text_pretrained_model = \"kykim/electra-kor-base\"\n",
    "if args.text_pretrained_model == \"koelectra\":\n",
    "    text_pretrained_model = \"monologg/koelectra-base-v3-discriminator\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(text_pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "with open(f'image_feature/X_img_{args.image_pretrained_model}_{image_size}.pkl', 'rb') as f:\n",
    "    X_img = pickle.load(f)\n",
    "with open(f'image_feature/X_test_img_{args.image_pretrained_model}_{image_size}.pkl', 'rb') as f:\n",
    "    X_test_img = pickle.load(f)\n",
    "    \n",
    "X_img.shape, X_test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def img_load(path):\n",
    "#     img = cv2.imread(path)[:,:,::-1]\n",
    "#     img = cv2.resize(img, (image_size, image_size))\n",
    "#     return img\n",
    "\n",
    "# X_img = np.array([img_load(i) for i in tqdm(glob('data/image/train/*.jpg'))])\n",
    "# X_test_img = np.array([img_load(i) for i in tqdm(glob('data/image/test/*.jpg'))])\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "X_txt = train[\"overview\"]\n",
    "X_test_txt = test[\"overview\"]\n",
    "\n",
    "y = train[\"cat3\"]\n",
    "y_encoder = {key : value for key, value in zip(np.unique(y), range(len(np.unique(y))))}\n",
    "y = np.array([y_encoder[k] for k in y])\n",
    "\n",
    "X_img.shape, X_test_img.shape, X_txt.shape, X_test_txt.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"len\"] = X_txt.apply(tokenizer.tokenize).apply(len)\n",
    "# test[\"len\"] = X_test_txt.apply(tokenizer.tokenize).apply(len)\n",
    "\n",
    "# train[\"len\"].median(), test[\"len\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image,\n",
    "        sentence,\n",
    "        labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.image = image\n",
    "        self.sentence = sentence\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.indexes = np.arange(len(self.sentence))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        \n",
    "        image = self.image[indexes]\n",
    "        sentence = self.sentence[indexes]\n",
    "\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=text_len,\n",
    "            return_tensors=\"tf\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [image, input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [image, input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(SEED).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img, X_val_img, y_train, y_val = train_test_split(X_img, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "X_train_txt, X_val_txt, _, _ = train_test_split(X_txt, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "X_train_img.shape, X_val_img.shape, X_train_txt.shape, X_val_txt.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataGenerator(\n",
    "    X_train_img,\n",
    "    X_train_txt.values, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_ds = DataGenerator(\n",
    "    X_val_img,\n",
    "    X_val_txt.values, y_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = layers.Input(\n",
    "    shape=(X_img.shape[1],), dtype=tf.float32, name=\"input_img\"\n",
    ")\n",
    "\n",
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"input_ids\"\n",
    ")\n",
    "attention_masks = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"attention_masks\"\n",
    ")\n",
    "token_type_ids = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"token_type_ids\"\n",
    ")\n",
    "\n",
    "txt_model = TFAutoModel.from_pretrained(text_pretrained_model, from_pt=True)\n",
    "txt_model.trainable = True\n",
    "\n",
    "output_txt = txt_model(\n",
    "    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    ")\n",
    "\n",
    "txt_side = output_txt.last_hidden_state\n",
    "txt_side = tf.keras.layers.GlobalAveragePooling1D()(txt_side)\n",
    "\n",
    "# x = layers.Concatenate()([input_img, txt_side])\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "\n",
    "img_side = layers.Reshape((-1, 1))(input_img)\n",
    "x = layers.Multiply()([img_side, txt_side])\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "output = layers.Dense(y_train.shape[1], activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_img, input_ids, attention_masks, token_type_ids], outputs=output\n",
    ")\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, decay_steps=1000)\n",
    "if args.optimizer == \"sgd\":\n",
    "    optim = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    \n",
    "if args.loss == \"cc\":\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=args.label_smoothing\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=loss_function,\n",
    "    metrics=tfa.metrics.F1Score(num_classes=y_train.shape[1], average=\"weighted\")\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"load_model/{parser.description}\"\n",
    "\n",
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"val_f1_score\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback, WandbCallback()],\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = history.history['f1_score']\n",
    "# val_acc = history.history['val_f1_score']\n",
    "\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# plt.plot(acc, label='Training Weighted-F1')\n",
    "# plt.plot(val_acc, label='Validation Weighted-F1')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Weighted-F1')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_weighted_f1 = model.evaluate(val_ds)[1]\n",
    "print(f\"val_weighted_f1: {val_weighted_f1}\")\n",
    "\n",
    "wandb.log({\n",
    "    'val_weighted_f1': val_weighted_f1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = DataGenerator(\n",
    "    X_test_img,\n",
    "    X_test_txt.values, None,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    include_targets=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = []\n",
    "for i in range(test_ds.__len__()):\n",
    "    pred_prob.append(model.predict(test_ds.__getitem__(i)))\n",
    "pred_prob = np.vstack(pred_prob)\n",
    "pred = np.argmax(pred_prob, axis=1)\n",
    "\n",
    "y_decoder = {value : key for key, value in y_encoder.items()}\n",
    "result = np.array([y_decoder[v] for v in pred])\n",
    "\n",
    "pd.Series(result).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission[\"cat3\"] = result\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
